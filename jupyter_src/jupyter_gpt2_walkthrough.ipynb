{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c19fca7-ef95-418a-8262-b20a3b79da62",
   "metadata": {},
   "source": [
    "## Standard module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d0afe5-ee78-41ec-86b7-af00fa6b04fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\Users\\WDAGUtilityAccount\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 70, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The paging file is too small for this operation to complete.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\WDAGUtilityAccount\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:70\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 70\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: The paging file is too small for this operation to complete.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpytorch\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any\n",
      "File \u001b[1;32mc:\\Users\\WDAGUtilityAccount\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\__init__.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlazy_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "File \u001b[1;32mc:\\Users\\WDAGUtilityAccount\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:85\u001b[0m\n\u001b[0;32m     83\u001b[0m     sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags)\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     86\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback\u001b[38;5;241m.\u001b[39mformat_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     87\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     88\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     89\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     90\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you need help, create an issue \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     92\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand include the entire stack trace above this error message.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\Users\\WDAGUtilityAccount\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 70, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: The paging file is too small for this operation to complete.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import regex\n",
    "import json\n",
    "import fire\n",
    "import torch as pytorch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e101c70b-a138-4ac3-bf36-ae8eeb7efe70",
   "metadata": {},
   "source": [
    "### Define the gpt_core_v2_class module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3dd6c6-8201-47c2-bd4b-aeebff3d7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt_core_v2_class:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    HPARAMS_117M = {\n",
    "    \"n_vocab\": 50257,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 12\n",
    "    }\n",
    "\n",
    "    HPARAMS_355M = {\n",
    "    \"n_vocab\": 50257,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 1024,\n",
    "    \"n_head\": 16,\n",
    "    \"n_layer\": 24\n",
    "    }\n",
    "\n",
    "    HPARAMS = { \"117M\": HPARAMS_117M,\n",
    "                \"355M\": HPARAMS_355M }\n",
    "\n",
    "    HPARAMS_PROTO = {\n",
    "    \"n_vocab\": 50257,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 12\n",
    "    }\n",
    "\n",
    "    # use deprecated HParams workaround\n",
    "    class HParams:\n",
    "        def __init__(self, **kwargs):\n",
    "            self._hparams = {}\n",
    "            for k, v in kwargs.items():\n",
    "                self.__validate_and_set__(pname=k, pvalue=v)\n",
    "\n",
    "        def __getattr__(self, pname):\n",
    "            if pname in self._hparams:\n",
    "                return self._hparams[pname]\n",
    "            raise AttributeError(f\"'HParams' object has no attribute '{pname}'\")\n",
    "\n",
    "        def __setattr__(self, pname, pvalue):\n",
    "            if pname in gpt_core_v2_class.HPARAMS_PROTO:\n",
    "                pval = gpt_core_v2_class.HPARAMS_PROTO[pname] if pvalue is None else pvalue\n",
    "                self._hparams[pname] = pval\n",
    "            super().__setattr__(pname, pvalue)\n",
    "\n",
    "        def __validate_and_set__(self, pname: str, pvalue: Any):\n",
    "            if pname in gpt_core_v2_class.HPARAMS_PROTO:\n",
    "                pval = gpt_core_v2_class.HPARAMS_PROTO[pname] if pvalue is None else pvalue\n",
    "                setattr(self, pname, pval)\n",
    "                self._hparams[pname] = pval\n",
    "            else:\n",
    "                raise ValueError(f\"Key [{pname}] not found in HParams\")\n",
    "\n",
    "        def override_from_dict(self, dict_):\n",
    "            for k, v in dict_.items():\n",
    "                self.__validate_and_set__(pname=k, pvalue=v)\n",
    "\n",
    "        @classmethod\n",
    "        def from_json(cls, json_file):\n",
    "            with open(json_file, 'r', encoding=\"UTF-8\") as f:\n",
    "                params = json.load(f)\n",
    "            return cls(**params)\n",
    "\n",
    "        def to_dict(self):\n",
    "            return self._hparams.copy()\n",
    "    \n",
    "    @classmethod\n",
    "    def default_hparams(cls):\n",
    "        return cls.HParams(\n",
    "            n_vocab=0,\n",
    "            n_ctx=1024,\n",
    "            n_embd=768,\n",
    "            n_head=12,\n",
    "            n_layer=12,\n",
    "        )\n",
    "\n",
    "    def shape_list(x):\n",
    "        \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "        static = x.shape.as_list()\n",
    "        dynamic = tf.shape(x)\n",
    "        return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "    def softmax(x, axis=-1):\n",
    "        return tf.nn.softmax(x, axis=axis)\n",
    "\n",
    "    def gelu(x):\n",
    "        return tf.nn.gelu(x)\n",
    "\n",
    "    #@tf.function\n",
    "    def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "        \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "        with tf.name_scope(scope):\n",
    "            n_state = x.shape[-1]\n",
    "            g = tf.Variable(tf.ones([n_state]), name='g')\n",
    "            b = tf.Variable(tf.zeros([n_state]), name='b')\n",
    "            u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
    "            s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
    "            x = (x - u) * tf.math.rsqrt(s + epsilon)\n",
    "            x = x*g + b\n",
    "            return x\n",
    "\n",
    "    def split_states(x, n):\n",
    "        \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "        *start, m = shape_list(x)\n",
    "        return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "    def merge_states(x):\n",
    "        \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "        *start, a, b = shape_list(x)\n",
    "        return tf.reshape(x, start + [a*b])\n",
    "\n",
    "    #@tf.function\n",
    "    def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "        with tf.name_scope(scope):\n",
    "            *start, nx = shape_list(x)\n",
    "            w = tf.Variable(tf.random.normal([1, nx, nf], stddev=w_init_stdev), name='w')\n",
    "            b = tf.Variable(tf.zeros([nf]), name='b')\n",
    "            c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "            return c\n",
    "\n",
    "    def attention_mask(nd, ns, *, dtype):\n",
    "        \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "        Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "        \"\"\"\n",
    "        i = tf.range(nd)[:,None]\n",
    "        j = tf.range(ns)\n",
    "        m = i >= j - ns + nd\n",
    "        return tf.cast(m, dtype)\n",
    "\n",
    "    #@tf.function\n",
    "    def attn(x, scope, n_state, *, past, hparams):\n",
    "        assert len(x.shape) == 3  # Should be [batch, sequence, features]\n",
    "        assert n_state % hparams.n_head == 0\n",
    "        if past is not None:\n",
    "            assert len(past.shape) == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "        def split_heads(x):\n",
    "            # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "            return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n",
    "\n",
    "        def merge_heads(x):\n",
    "            # Reverse of split_heads\n",
    "            return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "        def mask_attn_weights(w):\n",
    "            # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "            _, _, nd, ns = shape_list(w)\n",
    "            b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "            b = tf.reshape(b, [1, 1, nd, ns])\n",
    "            w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "            return w\n",
    "\n",
    "        def multihead_attn(q, k, v):\n",
    "            # q, k, v have shape [batch, heads, sequence, features]\n",
    "            w = tf.matmul(q, k, transpose_b=True)\n",
    "            w = w * tf.math.rsqrt(tf.cast(tf.shape(v)[-1], w.dtype))\n",
    "            w = mask_attn_weights(w)\n",
    "            w = softmax(w)\n",
    "            a = tf.matmul(w, v)\n",
    "            return a\n",
    "\n",
    "        with tf.name_scope(scope):\n",
    "            c = conv1d(x, 'c_attn', n_state*3)\n",
    "            q, k, v = tf.split(c, 3, axis=2)\n",
    "            q, k, v = map(split_heads, [q, k, v])\n",
    "            present = tf.stack([k, v], axis=1)\n",
    "            if past is not None:\n",
    "                pk, pv = tf.unstack(past, axis=1)\n",
    "                k = tf.concat([pk, k], axis=-2)\n",
    "                v = tf.concat([pv, v], axis=-2)\n",
    "            a = multihead_attn(q, k, v)\n",
    "            a = merge_heads(a)\n",
    "            a = conv1d(a, 'c_proj', n_state)\n",
    "            return a, present\n",
    "\n",
    "    #@tf.function\n",
    "    def mlp(x, scope, n_state, *, hparams=None):\n",
    "        with tf.name_scope(scope):\n",
    "            nx = tf.shape(x)[-1]\n",
    "            h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "            h2 = conv1d(h, 'c_proj', nx)\n",
    "            return h2\n",
    "\n",
    "    #@tf.function\n",
    "    def block(x, scope, *, past, hparams):\n",
    "        with tf.name_scope(scope):\n",
    "            nx = tf.shape(x)[-1]\n",
    "            a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "            x = x + a\n",
    "            m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "            x = x + m\n",
    "            return x, present\n",
    "\n",
    "    def past_shape(*, hparams, batch_size=None, sequence=None):\n",
    "        return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
    "\n",
    "    #@tf.function\n",
    "    def expand_tile(value, size):\n",
    "        \"\"\"Add a new axis of given size.\"\"\"\n",
    "        value = tf.convert_to_tensor(value, name='value')\n",
    "        ndims = tf.rank(value)\n",
    "        return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "    #@tf.function\n",
    "    def positions_for(tokens, past_length):\n",
    "        batch_size = tf.shape(tokens)[0]\n",
    "        nsteps = tf.shape(tokens)[1]\n",
    "        return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "\n",
    "    #@tf.function\n",
    "    def model(hparams, input_tokens, past=None, scope='model', reuse=True, seed=None):\n",
    "        with tf.name_scope(scope):\n",
    "            \n",
    "            if seed is not None:\n",
    "                np.random.seed(seed)\n",
    "                tf.random.set_seed(seed)\n",
    "\n",
    "            results = {}\n",
    "            batch, sequence = shape_list(input_tokens)\n",
    "\n",
    "            wpe = tf.Variable(tf.random.normal([hparams.n_ctx, hparams.n_embd], stddev=0.01), name='wpe')\n",
    "            wte = tf.Variable(tf.random.normal([hparams.n_vocab, hparams.n_embd], stddev=0.02), name='wte')\n",
    "\n",
    "            past_length = 0 if past is None else tf.shape(past)[-2]\n",
    "            h = tf.gather(wte, input_tokens) + tf.gather(wpe, positions_for(input_tokens, past_length))\n",
    "\n",
    "            # Transformer\n",
    "            presents = []\n",
    "            pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "            assert len(pasts) == hparams.n_layer\n",
    "            for layer, past in enumerate(pasts):\n",
    "                h, present = block(h, f'h{layer}', past=past, hparams=hparams)\n",
    "                presents.append(present)\n",
    "            results['present'] = tf.stack(presents, axis=1)\n",
    "            h = norm(h, 'ln_f')\n",
    "\n",
    "            # Language model loss.  Do tokens <n predict token n?\n",
    "            h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "            logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "            logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "            results['logits'] = logits\n",
    "            return results    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d689356-ab7e-4dfb-8f93-de72b5536309",
   "metadata": {},
   "source": [
    "### Define the codec_class module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d2d7b-c974-4ea5-82f2-2bdfcbd6b688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class codec_class:\n",
    "\n",
    "    @lru_cache()\n",
    "    def bytes_to_unicode():\n",
    "        \"\"\"\n",
    "        Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "        The reversible bpe codes work on unicode strings.\n",
    "        This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "        When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "        This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "        To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "        And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "        \"\"\"\n",
    "        bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "        cs = bs[:]\n",
    "        n = 0\n",
    "        for b in range(2**8):\n",
    "            if b not in bs:\n",
    "                bs.append(b)\n",
    "                cs.append(2**8+n)\n",
    "                n += 1\n",
    "        cs = [chr(n) for n in cs]\n",
    "        return dict(zip(bs, cs))\n",
    "\n",
    "    def get_pairs(word):\n",
    "        \"\"\"Return set of symbol pairs in a word.\n",
    "\n",
    "        Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "        \"\"\"\n",
    "        pairs = set()\n",
    "        prev_char = word[0]\n",
    "        for char in word[1:]:\n",
    "            pairs.add((prev_char, char))\n",
    "            prev_char = char\n",
    "        return pairs\n",
    "\n",
    "    class Encoder:\n",
    "        def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "            self.encoder = encoder\n",
    "            self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "            self.errors = errors # how to handle errors in decoding\n",
    "            self.byte_encoder = bytes_to_unicode()\n",
    "            self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "            self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "            self.cache = {}\n",
    "\n",
    "            # TODO: Add re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "            self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "        def bpe(self, token):\n",
    "            if token in self.cache:\n",
    "                return self.cache[token]\n",
    "            word = tuple(token)\n",
    "            pairs = get_pairs(word)\n",
    "\n",
    "            if not pairs:\n",
    "                return token\n",
    "\n",
    "            while True:\n",
    "                bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "                if bigram not in self.bpe_ranks:\n",
    "                    break\n",
    "                first, second = bigram\n",
    "                new_word = []\n",
    "                i = 0\n",
    "                while i < len(word):\n",
    "                    try:\n",
    "                        j = word.index(first, i)\n",
    "                        new_word.extend(word[i:j])\n",
    "                        i = j\n",
    "                    except Exception:\n",
    "                        new_word.extend(word[i:])\n",
    "                        break\n",
    "\n",
    "                    if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                        new_word.append(first+second)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word.append(word[i])\n",
    "                        i += 1\n",
    "                new_word = tuple(new_word)\n",
    "                word = new_word\n",
    "                if len(word) == 1:\n",
    "                    break\n",
    "                else:\n",
    "                    pairs = get_pairs(word)\n",
    "            word = ' '.join(word)\n",
    "            self.cache[token] = word\n",
    "            return word\n",
    "\n",
    "        def encode(self, text):\n",
    "            bpe_tokens = []\n",
    "            for token in re.findall(self.pat, text):\n",
    "                token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "                bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "            return bpe_tokens\n",
    "\n",
    "        def decode(self, tokens):\n",
    "            text = ''.join([self.decoder[token] for token in tokens])\n",
    "            text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=self.errors)\n",
    "            return text\n",
    "\n",
    "    def get_encoder(model_name, models_dir):\n",
    "        models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "        with open(os.path.join(models_dir, model_name, 'encoder.json'), 'r', encoding=\"utf-8\") as f:\n",
    "            encoder = json.load(f)\n",
    "        with open(os.path.join(models_dir, model_name, 'vocab.bpe'), 'r', encoding=\"utf-8\") as f:\n",
    "            bpe_data = f.read()\n",
    "        bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "        return Encoder(\n",
    "            encoder=encoder,\n",
    "            bpe_merges=bpe_merges,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048c2a2-9876-4166-b325-4d27adebbdd7",
   "metadata": {},
   "source": [
    "### Define the gpt_class module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41c7ecd-0fdf-4794-94de-778f36c4c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt_class:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    codec = codec_class()\n",
    "    gpt_core_v2 = gpt_core_v2_class()\n",
    "    \n",
    "    #@tf.function\n",
    "    def top_k_logits(logits, k):\n",
    "        \"\"\"top k logits\"\"\"\n",
    "        if k == 0:\n",
    "            # no truncation\n",
    "            return logits\n",
    "\n",
    "        def _top_k():\n",
    "            values, _ = tf.math.top_k(logits, k=k)\n",
    "            min_values = values[:, -1, tf.newaxis]\n",
    "            return tf.where(\n",
    "                logits < min_values,\n",
    "                tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "                logits,\n",
    "            )\n",
    "        return tf.cond(\n",
    "            tf.equal(k, 0),\n",
    "            lambda: logits,\n",
    "            lambda: _top_k(),\n",
    "        )\n",
    "\n",
    "    def get_codec(model_name, models_dir):\n",
    "        \"\"\"extend the codec encoder\"\"\"\n",
    "        return codec.get_encoder(model_name, models_dir)\n",
    "\n",
    "    def get_default_hparams(model_name, models_dir, fn):\n",
    "        \"\"\"default hparams\"\"\"\n",
    "        hparams = gpt_core_v2.default_hparams()\n",
    "        try:\n",
    "            models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "            with open(os.path.join(models_dir, model_name, fn), encoding=\"UTF-8\") as f:\n",
    "                hparams.override_from_dict(json.load(f))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return hparams\n",
    "\n",
    "    #@tf.function\n",
    "    def top_p_logits(logits, p):\n",
    "        \"\"\"Nucleus sampling\"\"\"\n",
    "        batch = tf.shape(logits)[0]\n",
    "        sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)\n",
    "        cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "        indices = tf.stack([\n",
    "            tf.range(0, batch),\n",
    "            # number of indices to include\n",
    "            tf.maximum(tf.reduce_sum(tf.cast(cumulative_probs <= p, tf.int32), axis=-1) - 1, 0),\n",
    "        ], axis=-1)\n",
    "        min_values = tf.gather_nd(sorted_logits, indices)\n",
    "        return tf.where(\n",
    "            logits < min_values[:, tf.newaxis],\n",
    "            tf.ones_like(logits) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "\n",
    "    #@tf.function\n",
    "    def submit_text_query(\n",
    "        hparams=gpt_core_v2.default_hparams(),\n",
    "        context=\"Hello, how are you today?\",\n",
    "        length=50,\n",
    "        model_name='124M',\n",
    "        models_dir='../models',\n",
    "        start_token=None,\n",
    "        batch_size=None,\n",
    "        temperature=1,\n",
    "        top_k=0,\n",
    "        top_p=1,\n",
    "        seed=None):\n",
    "\n",
    "        \"\"\"Submit a text query to the model\"\"\"\n",
    "        codec_instance = codec.get_encoder(model_name, models_dir)\n",
    "        context_tokens = codec_instance.encode(context) # [15496, 11, 703, 389, 345, 5633]\n",
    "        tokens_length = len(context_tokens)\n",
    "        if length is not None and tokens_length > length:\n",
    "            length = tokens_length\n",
    "        #context_tokens_tensor = tf.convert_to_tensor([context_tokens] * batch_size, dtype=tf.int32)\n",
    "        response_tokens = submit_token_query(\n",
    "            hparams=hparams,\n",
    "            context=context_tokens, #context_tokens_tensor,\n",
    "            length=length,\n",
    "            model_name=model_name,\n",
    "            models_dir=models_dir,\n",
    "            start_token=start_token,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            seed=seed)\n",
    "        \n",
    "        #generated = 0\n",
    "        #for i in range(batch_size):\n",
    "            #generated += 1\n",
    "            #text = codec_instance.decode(response_tokens[i])\n",
    "            #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            #print(text)\n",
    "\n",
    "        # Decode the generated response_tokens\n",
    "        text_response = codec_instance.decode(response_tokens[0])\n",
    "\n",
    "        return text_response\n",
    "\n",
    "    #@tf.function\n",
    "    def submit_token_query(\n",
    "        hparams=gpt_core_v2.default_hparams(),\n",
    "        length=50,\n",
    "        model_name='124M',\n",
    "        models_dir='../models',\n",
    "        start_token=None,\n",
    "        batch_size=None,\n",
    "        context=tf.convert_to_tensor([15496, 11, 703, 389, 345, 5633], dtype=tf.int32), #\"Hello, how are you today?\"\n",
    "        temperature=1,\n",
    "        top_k=0,\n",
    "        top_p=1,\n",
    "        seed=None\n",
    "    ):\n",
    "        \"\"\"submit a query to the model\"\"\"\n",
    "\n",
    "        if length is None:\n",
    "            length = hparams.n_ctx // 2\n",
    "        elif length > hparams.n_ctx:\n",
    "            raise ValueError(f\"Can't capture samples greater than the window size: {hparams.n_ctx}\")\n",
    "\n",
    "        if start_token is None:\n",
    "            assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "            # Ensure context is a TensorFlow tensor\n",
    "            context = tf.convert_to_tensor(context, dtype=tf.int32)\n",
    "            # Check the shape of context\n",
    "            context_shape = tf.shape(context)\n",
    "            if len(context.shape) == 1:\n",
    "                # If context is 1D, reshape it to 2D\n",
    "                context = tf.reshape(context, [1, -1])\n",
    "            elif len(context.shape) > 2:\n",
    "                raise ValueError(f\"Context should be 1D or 2D, but got shape {context.shape}\")\n",
    "            \n",
    "            # Set batch_size if it's not provided\n",
    "            if batch_size is None:\n",
    "                batch_size = context_shape[0]\n",
    "            else:\n",
    "                # If batch_size is provided, ensure it matches the context\n",
    "                tf.debugging.assert_equal(batch_size, context_shape[0], \n",
    "                                        message=\"Provided batch_size doesn't match context's first dimension\")\n",
    "        else:\n",
    "            assert context is None, 'Specify exactly one of start_token and context!'\n",
    "            context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "        def determine_length(provided_length=50, max_length=50):\n",
    "            if provided_length is None:\n",
    "                return max_length\n",
    "            if provided_length > max_length:\n",
    "                print(f\"Warning: Provided length ({provided_length}) exceeds maximum length ({max_length}). Using maximum length.\")\n",
    "                return max_length\n",
    "            return provided_length\n",
    "\n",
    "        length = determine_length(provided_length=length, max_length=hparams.n_ctx)\n",
    "\n",
    "        def step(hparams, tokens, past=None):\n",
    "            lm_output = gpt_core_v2.model(hparams=hparams, input_tokens=tokens, past=past, seed=seed)\n",
    "\n",
    "            logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "            presents = lm_output['present']\n",
    "            presents.set_shape(gpt_core_v2.past_shape(hparams=hparams, batch_size=batch_size))\n",
    "            return {\n",
    "                'logits': logits,\n",
    "                'presents': presents,\n",
    "            }\n",
    "\n",
    "        def body(past, prev, output):\n",
    "            next_outputs = step(hparams, prev, past=past)\n",
    "            logits = next_outputs['logits'][:, -1, :] / tf.cast(temperature, tf.float32)\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            logits = top_p_logits(logits, p=top_p)\n",
    "            samples = tf.random.categorical(logits, num_samples=1, dtype=tf.int32)\n",
    "            return [\n",
    "                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "                samples,\n",
    "                tf.concat([output, samples], axis=1)\n",
    "            ]\n",
    "\n",
    "        past, prev, output = body(None, context, context)\n",
    "\n",
    "        def cond(*args):\n",
    "            return True\n",
    "\n",
    "        _, _, tokens = tf.while_loop(\n",
    "            cond=cond, body=body,\n",
    "            maximum_iterations=length - 1,\n",
    "            loop_vars=[\n",
    "                past,\n",
    "                prev,\n",
    "                output\n",
    "            ],\n",
    "            shape_invariants=[\n",
    "                tf.TensorShape(gpt_core_v2.past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "                tf.TensorShape([batch_size, None]),\n",
    "                tf.TensorShape([batch_size, None]),\n",
    "            ],\n",
    "            back_prop=False,\n",
    "        )\n",
    "\n",
    "        return tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa98de1-4c28-49dd-be73-c343c7fcde98",
   "metadata": {},
   "source": [
    "### Define the Interactive_example_class module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "682dc49e-8fa6-40b3-87db-0a549e0365b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Interactive_example_class:\n",
    "\n",
    "    def interactive_model(\n",
    "        model_name='124M',\n",
    "        seed=None,\n",
    "        nsamples=1,\n",
    "        batch_size=None,\n",
    "        length=None,\n",
    "        temperature=1,\n",
    "        top_k=1,\n",
    "        top_p=1,\n",
    "        models_dir='../models' # Adjust as needed during DEBUG mode\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Interactively run the model\n",
    "        :model_name=124M : String, which model to use\n",
    "        :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "        results\n",
    "        :nsamples=1 : Number of samples to return total\n",
    "        :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "        :length=None : Number of tokens in generated text, if None (default), is\n",
    "        determined by model hyperparameters\n",
    "        :temperature=1 : Float value controlling randomness in boltzmann\n",
    "        distribution. Lower temperature results in less random completions. As the\n",
    "        temperature approaches zero, the model will become deterministic and\n",
    "        repetitive. Higher temperature results in more random completions.\n",
    "        :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "        considered for each step (token), resulting in deterministic completions,\n",
    "        while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "        special setting meaning no restrictions. 40 generally is a good value.\n",
    "        :models_dir : path to parent folder containing model subfolders\n",
    "        (i.e. contains the <model_name> folder)\n",
    "        \"\"\"\n",
    "\n",
    "        gpt = gpt_class()\n",
    "        \n",
    "        # Set up the seed for reproducibility\n",
    "        seed = 42  # Or whatever seed value you were using before\n",
    "\n",
    "        print(\"Initializing model...\")\n",
    "\n",
    "        # Interactive prompt loop\n",
    "        while True:\n",
    "            raw_text = input(\"Model prompt >>> \") # \"Hello, how are you ?\"\n",
    "            while not raw_text:\n",
    "                print('Please supply a text Prompt to the model!')\n",
    "                raw_text = input(\"Model prompt >>> \")\n",
    "\n",
    "            # Interactive example\n",
    "            # Increase nsamples to produce more generative examples\n",
    "            for i in range(nsamples):\n",
    "                text_output = gpt.submit_text_query(\n",
    "                    context = raw_text,\n",
    "                    length = length,\n",
    "                    batch_size = batch_size,\n",
    "                    temperature=temperature,\n",
    "                    top_k = top_k,\n",
    "                    top_p = top_p,\n",
    "                    models_dir = models_dir,\n",
    "                    model_name = model_name,\n",
    "                    seed = seed )\n",
    "                \n",
    "                print(\"=\" * 40 + f\" COMPLETION {i} \" + \"=\" * 40)\n",
    "                for _, text in enumerate(text_output):\n",
    "                    print(text)\n",
    "            \n",
    "            print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20cee5d7-d7d0-4886-87e5-5d5a0af4fe0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model prompt >>>  Hello, how are you ?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'codec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mInteractive_example_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minteractive_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 52\u001b[0m, in \u001b[0;36mInteractive_example_class.interactive_model\u001b[1;34m(model_name, seed, nsamples, batch_size, length, temperature, top_k, top_p, models_dir)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Interactive example\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Increase nsamples to produce more generative examples\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nsamples):\n\u001b[1;32m---> 52\u001b[0m     text_output \u001b[38;5;241m=\u001b[39m \u001b[43mgpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_text_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m COMPLETION \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_output):\n",
      "Cell \u001b[1;32mIn[3], line 76\u001b[0m, in \u001b[0;36mgpt_class.submit_text_query\u001b[1;34m(hparams, context, length, model_name, models_dir, start_token, batch_size, temperature, top_k, top_p, seed)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubmit_text_query\u001b[39m(\n\u001b[0;32m     63\u001b[0m     hparams\u001b[38;5;241m=\u001b[39mgpt_core_v2\u001b[38;5;241m.\u001b[39mdefault_hparams(),\n\u001b[0;32m     64\u001b[0m     context\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you today?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     72\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     73\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     75\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Submit a text query to the model\"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     codec_instance \u001b[38;5;241m=\u001b[39m \u001b[43mcodec\u001b[49m\u001b[38;5;241m.\u001b[39mget_encoder(model_name, models_dir)\n\u001b[0;32m     77\u001b[0m     context_tokens \u001b[38;5;241m=\u001b[39m codec_instance\u001b[38;5;241m.\u001b[39mencode(context) \u001b[38;5;66;03m# [15496, 11, 703, 389, 345, 5633]\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     tokens_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(context_tokens)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'codec' is not defined"
     ]
    }
   ],
   "source": [
    "Interactive_example_class.interactive_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
