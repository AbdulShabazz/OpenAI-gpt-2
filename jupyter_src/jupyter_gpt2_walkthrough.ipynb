{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c19fca7-ef95-418a-8262-b20a3b79da62",
   "metadata": {},
   "source": [
    "## Standard module imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38d0afe5-ee78-41ec-86b7-af00fa6b04fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex\n",
    "import json\n",
    "import fire\n",
    "import torch as pytorch\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e101c70b-a138-4ac3-bf36-ae8eeb7efe70",
   "metadata": {},
   "source": [
    "### Define the gpt_core_v2_class module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb3dd6c6-8201-47c2-bd4b-aeebff3d7864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt_core_v2_class:\n",
    "    HPARAMS_117M = {\n",
    "    \"n_vocab\": 50257,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 12\n",
    "    }\n",
    "\n",
    "    HPARAMS_355M = {\n",
    "    \"n_vocab\": 50257,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 1024,\n",
    "    \"n_head\": 16,\n",
    "    \"n_layer\": 24\n",
    "    }\n",
    "\n",
    "    HPARAMS = { \"117M\": HPARAMS_117M,\n",
    "                \"355M\": HPARAMS_355M }\n",
    "\n",
    "    HPARAMS_PROTO = {\n",
    "    \"n_vocab\": 50257,\n",
    "    \"n_ctx\": 1024,\n",
    "    \"n_embd\": 768,\n",
    "    \"n_head\": 12,\n",
    "    \"n_layer\": 12\n",
    "    }\n",
    "\n",
    "    # use deprecated HParams workaround\n",
    "    class HParams:\n",
    "        def __init__(self, **kwargs):\n",
    "            self._hparams = {}\n",
    "            for k, v in kwargs.items():\n",
    "                self.__validate_and_set__(pname=k, pvalue=v)\n",
    "\n",
    "        def __getattr__(self, pname):\n",
    "            if pname in self._hparams:\n",
    "                return self._hparams[pname]\n",
    "            raise AttributeError(f\"'HParams' object has no attribute '{pname}'\")\n",
    "\n",
    "        def __setattr__(self, pname, pvalue):\n",
    "            if pname in gpt_core_v2_class.HPARAMS_PROTO:\n",
    "                pval = gpt_core_v2_class.HPARAMS_PROTO[pname] if pvalue is None else pvalue\n",
    "                self._hparams[pname] = pval\n",
    "            super().__setattr__(pname, pvalue)\n",
    "\n",
    "        def __validate_and_set__(self, pname: str, pvalue: Any):\n",
    "            if pname in gpt_core_v2_class.HPARAMS_PROTO:\n",
    "                pval = gpt_core_v2_class.HPARAMS_PROTO[pname] if pvalue is None else pvalue\n",
    "                setattr(self, pname, pval)\n",
    "                self._hparams[pname] = pval\n",
    "            else:\n",
    "                raise ValueError(f\"Key [{pname}] not found in HParams\")\n",
    "\n",
    "        def override_from_dict(self, dict_):\n",
    "            for k, v in dict_.items():\n",
    "                self.__validate_and_set__(pname=k, pvalue=v)\n",
    "\n",
    "        @classmethod\n",
    "        def from_json(cls, json_file):\n",
    "            with open(json_file, 'r', encoding=\"UTF-8\") as f:\n",
    "                params = json.load(f)\n",
    "            return cls(**params)\n",
    "\n",
    "        def to_dict(self):\n",
    "            return self._hparams.copy()\n",
    "    \n",
    "    @classmethod\n",
    "    def default_hparams(cls):\n",
    "        return cls.HParams(\n",
    "            n_vocab=0,\n",
    "            n_ctx=1024,\n",
    "            n_embd=768,\n",
    "            n_head=12,\n",
    "            n_layer=12,\n",
    "        )\n",
    "\n",
    "    def shape_list(x):\n",
    "        \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
    "        static = x.shape.as_list()\n",
    "        dynamic = tf.shape(x)\n",
    "        return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
    "\n",
    "    def softmax(x, axis=-1):\n",
    "        return tf.nn.softmax(x, axis=axis)\n",
    "\n",
    "    def gelu(x):\n",
    "        return tf.nn.gelu(x)\n",
    "\n",
    "    #@tf.function\n",
    "    def norm(x, scope, *, axis=-1, epsilon=1e-5):\n",
    "        \"\"\"Normalize to mean = 0, std = 1, then do a diagonal affine transform.\"\"\"\n",
    "        with tf.name_scope(scope):\n",
    "            n_state = x.shape[-1]\n",
    "            g = tf.Variable(tf.ones([n_state]), name='g')\n",
    "            b = tf.Variable(tf.zeros([n_state]), name='b')\n",
    "            u = tf.reduce_mean(x, axis=axis, keepdims=True)\n",
    "            s = tf.reduce_mean(tf.square(x-u), axis=axis, keepdims=True)\n",
    "            x = (x - u) * tf.math.rsqrt(s + epsilon)\n",
    "            x = x*g + b\n",
    "            return x\n",
    "\n",
    "    def split_states(x, n):\n",
    "        \"\"\"Reshape the last dimension of x into [n, x.shape[-1]/n].\"\"\"\n",
    "        *start, m = shape_list(x)\n",
    "        return tf.reshape(x, start + [n, m//n])\n",
    "\n",
    "    def merge_states(x):\n",
    "        \"\"\"Smash the last two dimensions of x into a single dimension.\"\"\"\n",
    "        *start, a, b = shape_list(x)\n",
    "        return tf.reshape(x, start + [a*b])\n",
    "\n",
    "    #@tf.function\n",
    "    def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
    "        with tf.name_scope(scope):\n",
    "            *start, nx = shape_list(x)\n",
    "            w = tf.Variable(tf.random.normal([1, nx, nf], stddev=w_init_stdev), name='w')\n",
    "            b = tf.Variable(tf.zeros([nf]), name='b')\n",
    "            c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
    "            return c\n",
    "\n",
    "    def attention_mask(nd, ns, *, dtype):\n",
    "        \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
    "        Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
    "        \"\"\"\n",
    "        i = tf.range(nd)[:,None]\n",
    "        j = tf.range(ns)\n",
    "        m = i >= j - ns + nd\n",
    "        return tf.cast(m, dtype)\n",
    "\n",
    "    #@tf.function\n",
    "    def attn(x, scope, n_state, *, past, hparams):\n",
    "        assert len(x.shape) == 3  # Should be [batch, sequence, features]\n",
    "        assert n_state % hparams.n_head == 0\n",
    "        if past is not None:\n",
    "            assert len(past.shape) == 5  # Should be [batch, 2, heads, sequence, features], where 2 is [k, v]\n",
    "\n",
    "        def split_heads(x):\n",
    "            # From [batch, sequence, features] to [batch, heads, sequence, features]\n",
    "            return tf.transpose(split_states(x, hparams.n_head), [0, 2, 1, 3])\n",
    "\n",
    "        def merge_heads(x):\n",
    "            # Reverse of split_heads\n",
    "            return merge_states(tf.transpose(x, [0, 2, 1, 3]))\n",
    "\n",
    "        def mask_attn_weights(w):\n",
    "            # w has shape [batch, heads, dst_sequence, src_sequence], where information flows from src to dst.\n",
    "            _, _, nd, ns = shape_list(w)\n",
    "            b = attention_mask(nd, ns, dtype=w.dtype)\n",
    "            b = tf.reshape(b, [1, 1, nd, ns])\n",
    "            w = w*b - tf.cast(1e10, w.dtype)*(1-b)\n",
    "            return w\n",
    "\n",
    "        def multihead_attn(q, k, v):\n",
    "            # q, k, v have shape [batch, heads, sequence, features]\n",
    "            w = tf.matmul(q, k, transpose_b=True)\n",
    "            w = w * tf.math.rsqrt(tf.cast(tf.shape(v)[-1], w.dtype))\n",
    "            w = mask_attn_weights(w)\n",
    "            w = softmax(w)\n",
    "            a = tf.matmul(w, v)\n",
    "            return a\n",
    "\n",
    "        with tf.name_scope(scope):\n",
    "            c = conv1d(x, 'c_attn', n_state*3)\n",
    "            q, k, v = tf.split(c, 3, axis=2)\n",
    "            q, k, v = map(split_heads, [q, k, v])\n",
    "            present = tf.stack([k, v], axis=1)\n",
    "            if past is not None:\n",
    "                pk, pv = tf.unstack(past, axis=1)\n",
    "                k = tf.concat([pk, k], axis=-2)\n",
    "                v = tf.concat([pv, v], axis=-2)\n",
    "            a = multihead_attn(q, k, v)\n",
    "            a = merge_heads(a)\n",
    "            a = conv1d(a, 'c_proj', n_state)\n",
    "            return a, present\n",
    "\n",
    "    #@tf.function\n",
    "    def mlp(x, scope, n_state, *, hparams=None):\n",
    "        with tf.name_scope(scope):\n",
    "            nx = tf.shape(x)[-1]\n",
    "            h = gelu(conv1d(x, 'c_fc', n_state))\n",
    "            h2 = conv1d(h, 'c_proj', nx)\n",
    "            return h2\n",
    "\n",
    "    #@tf.function\n",
    "    def block(x, scope, *, past, hparams):\n",
    "        with tf.name_scope(scope):\n",
    "            nx = tf.shape(x)[-1]\n",
    "            a, present = attn(norm(x, 'ln_1'), 'attn', nx, past=past, hparams=hparams)\n",
    "            x = x + a\n",
    "            m = mlp(norm(x, 'ln_2'), 'mlp', nx*4, hparams=hparams)\n",
    "            x = x + m\n",
    "            return x, present\n",
    "\n",
    "    def past_shape(*, hparams, batch_size=None, sequence=None):\n",
    "        return [batch_size, hparams.n_layer, 2, hparams.n_head, sequence, hparams.n_embd // hparams.n_head]\n",
    "\n",
    "    #@tf.function\n",
    "    def expand_tile(value, size):\n",
    "        \"\"\"Add a new axis of given size.\"\"\"\n",
    "        value = tf.convert_to_tensor(value, name='value')\n",
    "        ndims = tf.rank(value)\n",
    "        return tf.tile(tf.expand_dims(value, axis=0), [size] + [1]*ndims)\n",
    "\n",
    "    #@tf.function\n",
    "    def positions_for(tokens, past_length):\n",
    "        batch_size = tf.shape(tokens)[0]\n",
    "        nsteps = tf.shape(tokens)[1]\n",
    "        return expand_tile(past_length + tf.range(nsteps), batch_size)\n",
    "\n",
    "    #@tf.function\n",
    "    def model(hparams, input_tokens, past=None, scope='model', reuse=True, seed=None):\n",
    "        with tf.name_scope(scope):\n",
    "            \n",
    "            if seed is not None:\n",
    "                np.random.seed(seed)\n",
    "                tf.random.set_seed(seed)\n",
    "\n",
    "            results = {}\n",
    "            batch, sequence = shape_list(input_tokens)\n",
    "\n",
    "            wpe = tf.Variable(tf.random.normal([hparams.n_ctx, hparams.n_embd], stddev=0.01), name='wpe')\n",
    "            wte = tf.Variable(tf.random.normal([hparams.n_vocab, hparams.n_embd], stddev=0.02), name='wte')\n",
    "\n",
    "            past_length = 0 if past is None else tf.shape(past)[-2]\n",
    "            h = tf.gather(wte, input_tokens) + tf.gather(wpe, positions_for(input_tokens, past_length))\n",
    "\n",
    "            # Transformer\n",
    "            presents = []\n",
    "            pasts = tf.unstack(past, axis=1) if past is not None else [None] * hparams.n_layer\n",
    "            assert len(pasts) == hparams.n_layer\n",
    "            for layer, past in enumerate(pasts):\n",
    "                h, present = block(h, f'h{layer}', past=past, hparams=hparams)\n",
    "                presents.append(present)\n",
    "            results['present'] = tf.stack(presents, axis=1)\n",
    "            h = norm(h, 'ln_f')\n",
    "\n",
    "            # Language model loss.  Do tokens <n predict token n?\n",
    "            h_flat = tf.reshape(h, [batch*sequence, hparams.n_embd])\n",
    "            logits = tf.matmul(h_flat, wte, transpose_b=True)\n",
    "            logits = tf.reshape(logits, [batch, sequence, hparams.n_vocab])\n",
    "            results['logits'] = logits\n",
    "            return results    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d048c2a2-9876-4166-b325-4d27adebbdd7",
   "metadata": {},
   "source": [
    "### Define the gpt_class module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41c7ecd-0fdf-4794-94de-778f36c4c196",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gpt_class:\n",
    "    \n",
    "    gpt_core_v2 = gpt_core_v2_class()\n",
    "    \n",
    "    #@tf.function\n",
    "    def top_k_logits(logits, k):\n",
    "        \"\"\"top k logits\"\"\"\n",
    "        if k == 0:\n",
    "            # no truncation\n",
    "            return logits\n",
    "\n",
    "        def _top_k():\n",
    "            values, _ = tf.math.top_k(logits, k=k)\n",
    "            min_values = values[:, -1, tf.newaxis]\n",
    "            return tf.where(\n",
    "                logits < min_values,\n",
    "                tf.ones_like(logits, dtype=logits.dtype) * -1e10,\n",
    "                logits,\n",
    "            )\n",
    "        return tf.cond(\n",
    "            tf.equal(k, 0),\n",
    "            lambda: logits,\n",
    "            lambda: _top_k(),\n",
    "        )\n",
    "\n",
    "    def get_codec(model_name, models_dir):\n",
    "        \"\"\"extend the codec encoder\"\"\"\n",
    "        return codec.get_encoder(model_name, models_dir)\n",
    "\n",
    "    def get_default_hparams(model_name, models_dir, fn):\n",
    "        \"\"\"default hparams\"\"\"\n",
    "        hparams = gpt_core_v2.default_hparams()\n",
    "        try:\n",
    "            models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "            with open(os.path.join(models_dir, model_name, fn), encoding=\"UTF-8\") as f:\n",
    "                hparams.override_from_dict(json.load(f))\n",
    "        except Exception:\n",
    "            pass\n",
    "        return hparams\n",
    "\n",
    "    #@tf.function\n",
    "    def top_p_logits(logits, p):\n",
    "        \"\"\"Nucleus sampling\"\"\"\n",
    "        batch = tf.shape(logits)[0]\n",
    "        sorted_logits = tf.sort(logits, direction='DESCENDING', axis=-1)\n",
    "        cumulative_probs = tf.cumsum(tf.nn.softmax(sorted_logits, axis=-1), axis=-1)\n",
    "        indices = tf.stack([\n",
    "            tf.range(0, batch),\n",
    "            # number of indices to include\n",
    "            tf.maximum(tf.reduce_sum(tf.cast(cumulative_probs <= p, tf.int32), axis=-1) - 1, 0),\n",
    "        ], axis=-1)\n",
    "        min_values = tf.gather_nd(sorted_logits, indices)\n",
    "        return tf.where(\n",
    "            logits < min_values[:, tf.newaxis],\n",
    "            tf.ones_like(logits) * -1e10,\n",
    "            logits,\n",
    "        )\n",
    "\n",
    "    #@tf.function\n",
    "    def submit_text_query(\n",
    "        hparams=gpt_core_v2.default_hparams(),\n",
    "        context=\"Hello, how are you today?\",\n",
    "        length=50,\n",
    "        model_name='124M',\n",
    "        models_dir='../models',\n",
    "        start_token=None,\n",
    "        batch_size=None,\n",
    "        temperature=1,\n",
    "        top_k=0,\n",
    "        top_p=1,\n",
    "        seed=None):\n",
    "\n",
    "        \"\"\"Submit a text query to the model\"\"\"\n",
    "        codec_instance = codec.get_encoder(model_name, models_dir)\n",
    "        context_tokens = codec_instance.encode(context) # [15496, 11, 703, 389, 345, 5633]\n",
    "        tokens_length = len(context_tokens)\n",
    "        if length is not None and tokens_length > length:\n",
    "            length = tokens_length\n",
    "        #context_tokens_tensor = tf.convert_to_tensor([context_tokens] * batch_size, dtype=tf.int32)\n",
    "        response_tokens = submit_token_query(\n",
    "            hparams=hparams,\n",
    "            context=context_tokens, #context_tokens_tensor,\n",
    "            length=length,\n",
    "            model_name=model_name,\n",
    "            models_dir=models_dir,\n",
    "            start_token=start_token,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            seed=seed)\n",
    "        \n",
    "        #generated = 0\n",
    "        #for i in range(batch_size):\n",
    "            #generated += 1\n",
    "            #text = codec_instance.decode(response_tokens[i])\n",
    "            #print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "            #print(text)\n",
    "\n",
    "        # Decode the generated response_tokens\n",
    "        text_response = codec_instance.decode(response_tokens[0])\n",
    "\n",
    "        return text_response\n",
    "\n",
    "    #@tf.function\n",
    "    def submit_token_query(\n",
    "        hparams=gpt_core_v2.default_hparams(),\n",
    "        length=50,\n",
    "        model_name='124M',\n",
    "        models_dir='../models',\n",
    "        start_token=None,\n",
    "        batch_size=None,\n",
    "        context=tf.convert_to_tensor([15496, 11, 703, 389, 345, 5633], dtype=tf.int32), #\"Hello, how are you today?\"\n",
    "        temperature=1,\n",
    "        top_k=0,\n",
    "        top_p=1,\n",
    "        seed=None\n",
    "    ):\n",
    "        \"\"\"submit a query to the model\"\"\"\n",
    "\n",
    "        if length is None:\n",
    "            length = hparams.n_ctx // 2\n",
    "        elif length > hparams.n_ctx:\n",
    "            raise ValueError(f\"Can't capture samples greater than the window size: {hparams.n_ctx}\")\n",
    "\n",
    "        if start_token is None:\n",
    "            assert context is not None, 'Specify exactly one of start_token and context!'\n",
    "            # Ensure context is a TensorFlow tensor\n",
    "            context = tf.convert_to_tensor(context, dtype=tf.int32)\n",
    "            # Check the shape of context\n",
    "            context_shape = tf.shape(context)\n",
    "            if len(context.shape) == 1:\n",
    "                # If context is 1D, reshape it to 2D\n",
    "                context = tf.reshape(context, [1, -1])\n",
    "            elif len(context.shape) > 2:\n",
    "                raise ValueError(f\"Context should be 1D or 2D, but got shape {context.shape}\")\n",
    "            \n",
    "            # Set batch_size if it's not provided\n",
    "            if batch_size is None:\n",
    "                batch_size = context_shape[0]\n",
    "            else:\n",
    "                # If batch_size is provided, ensure it matches the context\n",
    "                tf.debugging.assert_equal(batch_size, context_shape[0], \n",
    "                                        message=\"Provided batch_size doesn't match context's first dimension\")\n",
    "        else:\n",
    "            assert context is None, 'Specify exactly one of start_token and context!'\n",
    "            context = tf.fill([batch_size, 1], start_token)\n",
    "\n",
    "        def determine_length(provided_length=50, max_length=50):\n",
    "            if provided_length is None:\n",
    "                return max_length\n",
    "            if provided_length > max_length:\n",
    "                print(f\"Warning: Provided length ({provided_length}) exceeds maximum length ({max_length}). Using maximum length.\")\n",
    "                return max_length\n",
    "            return provided_length\n",
    "\n",
    "        length = determine_length(provided_length=length, max_length=hparams.n_ctx)\n",
    "\n",
    "        def step(hparams, tokens, past=None):\n",
    "            lm_output = gpt_core_v2.model(hparams=hparams, input_tokens=tokens, past=past, seed=seed)\n",
    "\n",
    "            logits = lm_output['logits'][:, :, :hparams.n_vocab]\n",
    "            presents = lm_output['present']\n",
    "            presents.set_shape(gpt_core_v2.past_shape(hparams=hparams, batch_size=batch_size))\n",
    "            return {\n",
    "                'logits': logits,\n",
    "                'presents': presents,\n",
    "            }\n",
    "\n",
    "        def body(past, prev, output):\n",
    "            next_outputs = step(hparams, prev, past=past)\n",
    "            logits = next_outputs['logits'][:, -1, :] / tf.cast(temperature, tf.float32)\n",
    "            logits = top_k_logits(logits, k=top_k)\n",
    "            logits = top_p_logits(logits, p=top_p)\n",
    "            samples = tf.random.categorical(logits, num_samples=1, dtype=tf.int32)\n",
    "            return [\n",
    "                next_outputs['presents'] if past is None else tf.concat([past, next_outputs['presents']], axis=-2),\n",
    "                samples,\n",
    "                tf.concat([output, samples], axis=1)\n",
    "            ]\n",
    "\n",
    "        past, prev, output = body(None, context, context)\n",
    "\n",
    "        def cond(*args):\n",
    "            return True\n",
    "\n",
    "        _, _, tokens = tf.while_loop(\n",
    "            cond=cond, body=body,\n",
    "            maximum_iterations=length - 1,\n",
    "            loop_vars=[\n",
    "                past,\n",
    "                prev,\n",
    "                output\n",
    "            ],\n",
    "            shape_invariants=[\n",
    "                tf.TensorShape(gpt_core_v2.past_shape(hparams=hparams, batch_size=batch_size)),\n",
    "                tf.TensorShape([batch_size, None]),\n",
    "                tf.TensorShape([batch_size, None]),\n",
    "            ],\n",
    "            back_prop=False,\n",
    "        )\n",
    "\n",
    "        return tokens    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa98de1-4c28-49dd-be73-c343c7fcde98",
   "metadata": {},
   "source": [
    "### Define the Interactive_example_class module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "682dc49e-8fa6-40b3-87db-0a549e0365b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Model prompt >>>  Hello, how are you ?\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'gpt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mInteractive_example_class\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43minteractive_model\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m124M\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodels_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../models\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Adjust as needed during DEBUG mode\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250;43m        \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;43;03m        Interactively run the model\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;43;03m        :model_name=124M : String, which model to use\u001b[39;49;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124;43;03m        (i.e. contains the <model_name> folder)\u001b[39;49;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;43;03m        \"\"\"\u001b[39;49;00m\n",
      "Cell \u001b[1;32mIn[4], line 68\u001b[0m, in \u001b[0;36mInteractive_example_class\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 68\u001b[0m     \u001b[43mfire\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFire\u001b[49m\u001b[43m(\u001b[49m\u001b[43minteractive_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fire\\core.py:143\u001b[0m, in \u001b[0;36mFire\u001b[1;34m(component, command, name, serialize)\u001b[0m\n\u001b[0;32m    140\u001b[0m   context\u001b[38;5;241m.\u001b[39mupdate(caller_globals)\n\u001b[0;32m    141\u001b[0m   context\u001b[38;5;241m.\u001b[39mupdate(caller_locals)\n\u001b[1;32m--> 143\u001b[0m component_trace \u001b[38;5;241m=\u001b[39m \u001b[43m_Fire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparsed_flag_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m component_trace\u001b[38;5;241m.\u001b[39mHasError():\n\u001b[0;32m    146\u001b[0m   _DisplayError(component_trace)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fire\\core.py:477\u001b[0m, in \u001b[0;36m_Fire\u001b[1;34m(component, args, parsed_flag_args, context, name)\u001b[0m\n\u001b[0;32m    474\u001b[0m is_class \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39misclass(component)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 477\u001b[0m   component, remaining_args \u001b[38;5;241m=\u001b[39m \u001b[43m_CallAndUpdateTrace\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomponent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m      \u001b[49m\u001b[43mremaining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcomponent_trace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtreatment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroutine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomponent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    483\u001b[0m   handled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m FireError \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fire\\core.py:693\u001b[0m, in \u001b[0;36m_CallAndUpdateTrace\u001b[1;34m(component, args, component_trace, treatment, target)\u001b[0m\n\u001b[0;32m    691\u001b[0m   component \u001b[38;5;241m=\u001b[39m loop\u001b[38;5;241m.\u001b[39mrun_until_complete(fn(\u001b[38;5;241m*\u001b[39mvarargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    692\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 693\u001b[0m   component \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvarargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m treatment \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    696\u001b[0m   action \u001b[38;5;241m=\u001b[39m trace\u001b[38;5;241m.\u001b[39mINSTANTIATED_CLASS\n",
      "Cell \u001b[1;32mIn[4], line 49\u001b[0m, in \u001b[0;36mInteractive_example_class.interactive_model\u001b[1;34m(model_name, seed, nsamples, batch_size, length, temperature, top_k, top_p, models_dir)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Interactive example\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Increase nsamples to produce more generative examples\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nsamples):\n\u001b[1;32m---> 49\u001b[0m     text_output \u001b[38;5;241m=\u001b[39m \u001b[43mgpt\u001b[49m\u001b[38;5;241m.\u001b[39msubmit_text_query(\n\u001b[0;32m     50\u001b[0m         context \u001b[38;5;241m=\u001b[39m raw_text,\n\u001b[0;32m     51\u001b[0m         length \u001b[38;5;241m=\u001b[39m length,\n\u001b[0;32m     52\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m batch_size,\n\u001b[0;32m     53\u001b[0m         temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m     54\u001b[0m         top_k \u001b[38;5;241m=\u001b[39m top_k,\n\u001b[0;32m     55\u001b[0m         top_p \u001b[38;5;241m=\u001b[39m top_p,\n\u001b[0;32m     56\u001b[0m         models_dir \u001b[38;5;241m=\u001b[39m models_dir,\n\u001b[0;32m     57\u001b[0m         model_name \u001b[38;5;241m=\u001b[39m model_name,\n\u001b[0;32m     58\u001b[0m         seed \u001b[38;5;241m=\u001b[39m seed )\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m COMPLETION \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(text_output):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'gpt' is not defined"
     ]
    }
   ],
   "source": [
    "class Interactive_example_class:\n",
    "    def interactive_model(\n",
    "        model_name='124M',\n",
    "        seed=None,\n",
    "        nsamples=1,\n",
    "        batch_size=None,\n",
    "        length=None,\n",
    "        temperature=1,\n",
    "        top_k=1,\n",
    "        top_p=1,\n",
    "        models_dir='../models' # Adjust as needed during DEBUG mode\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Interactively run the model\n",
    "        :model_name=124M : String, which model to use\n",
    "        :seed=None : Integer seed for random number generators, fix seed to reproduce\n",
    "        results\n",
    "        :nsamples=1 : Number of samples to return total\n",
    "        :batch_size=1 : Number of batches (only affects speed/memory).  Must divide nsamples.\n",
    "        :length=None : Number of tokens in generated text, if None (default), is\n",
    "        determined by model hyperparameters\n",
    "        :temperature=1 : Float value controlling randomness in boltzmann\n",
    "        distribution. Lower temperature results in less random completions. As the\n",
    "        temperature approaches zero, the model will become deterministic and\n",
    "        repetitive. Higher temperature results in more random completions.\n",
    "        :top_k=0 : Integer value controlling diversity. 1 means only 1 word is\n",
    "        considered for each step (token), resulting in deterministic completions,\n",
    "        while 40 means 40 words are considered at each step. 0 (default) is a\n",
    "        special setting meaning no restrictions. 40 generally is a good value.\n",
    "        :models_dir : path to parent folder containing model subfolders\n",
    "        (i.e. contains the <model_name> folder)\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up the seed for reproducibility\n",
    "        seed = 42  # Or whatever seed value you were using before\n",
    "\n",
    "        print(\"Initializing model...\")\n",
    "\n",
    "        # Interactive prompt loop\n",
    "        while True:\n",
    "            raw_text = input(\"Model prompt >>> \") # \"Hello, how are you ?\"\n",
    "            while not raw_text:\n",
    "                print('Please supply a text Prompt to the model!')\n",
    "                raw_text = input(\"Model prompt >>> \")\n",
    "\n",
    "            # Interactive example\n",
    "            # Increase nsamples to produce more generative examples\n",
    "            for i in range(nsamples):\n",
    "                text_output = gpt.submit_text_query(\n",
    "                    context = raw_text,\n",
    "                    length = length,\n",
    "                    batch_size = batch_size,\n",
    "                    temperature=temperature,\n",
    "                    top_k = top_k,\n",
    "                    top_p = top_p,\n",
    "                    models_dir = models_dir,\n",
    "                    model_name = model_name,\n",
    "                    seed = seed )\n",
    "                \n",
    "                print(\"=\" * 40 + f\" COMPLETION {i} \" + \"=\" * 40)\n",
    "                for _, text in enumerate(text_output):\n",
    "                    print(text)\n",
    "            \n",
    "            print(\"=\" * 80)\n",
    "\n",
    "\n",
    "    if __name__ == '__main__':\n",
    "        fire.Fire(interactive_model)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
